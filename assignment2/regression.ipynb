{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to second assigment\n",
    "\n",
    "Welcome to the second assignment of this machine learning course. Today, we will learn a bit more about working with data (loading/preprocessing/displaying/\\*ing...) and our first model: **Linear Regression**.\n",
    "\n",
    "The purpose of this assignment is to give you a chance to implement your **own regression model**. First, we will start with **Linear Regression with one variable**. As usual, we will need to deal with some necessary imports first. In this exercise we will work with `numpy`, `matplotlib`, and `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine you own a food truck and you colected a bunch of data on how well you did (what profit you had) in different cities. You also know the population of these cities. As an efficient owner of a food truck, you would like to predict your future path and know where to head next. \n",
    "\n",
    "In the variable `data` we loaded exactly this data for you. The first column is the **population** of a city and the second column is the **profit** of a food truck in that city (a negative value for profit indicates a loss). \n",
    "\n",
    "As you can see `numpy` provides a handy function `loadtxt()` (*check out official docs for more info*) that can load almost any type of common data file. Before working with any data, it is usually very useful to see and briefly examine it. A quick way of doing that usually boils down to drawing a plot, which we also do with the food truck data. On X axis we have the population of a given city and on Y axis we have the profit. (*We also suggest you look at the raw data and take a peak inside the actual raw data file.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('food_truck_data.csv', delimiter=',')\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "plt.xlim(4)\n",
    "plt.xlabel('Population of City in 10 000s')\n",
    "plt.ylabel('Profit in $10 000s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to fit a *linear regression* model to this data. More formaly, we are going to search for a hypothesis $h \\in H$ which minimizes the cost function $$J(\\theta) = \\sum_{i=1}^t{err(h_{\\theta}(x_i), y_i)}$$\n",
    "where $t$ is the number of examples, $x_i$ is $i$th example, $y_i$ is $i$th target value, the function $h_{\\theta}$ is described as $$h_{\\theta}(x) = \\theta_0 + \\theta_1x$$ and $err(\\hat{y}, y)$ is an error function which measures the error that the model made. \n",
    "\n",
    "In our example the error function will be **Mean Squared Error** so we will try to find parameters $\\theta$ that minimize\n",
    "$$J(\\theta) = \\frac{1}{2t}\\sum_{i=1}^t(h_{\\theta}(x_i) - y_i)^2$$\n",
    "\n",
    "You may recall from the lectures that one way of doing this is to use an algorithm called **gradient descent**. In each step of gradient descent you will perform a parameter update so that with each step your parameters will come closer to the optimal values.\n",
    "\n",
    "\n",
    "Putting all of this together, our parameter update now looks as follows:\n",
    "$$\\theta_j = \\theta_j - \\alpha\\frac{1}{t}\\sum_{i=1}^{t}(h_{\\theta}(x_i) - y_i)x_{i,j}$$\n",
    "where $t$ is the number of examples, $\\alpha$ is learning rate, $x_i$ is $i$th example, $y_i$ is $i$th target value and $x_{i, j}$ is $i$th example's $j$th feature.\n",
    "\n",
    "In the next cell you can find some initial setup of some important variables. We initilize the values of $\\theta$ to zero and set our learning rate and number of interations. We, also separated target variables into separate variable `y`. Please, set up your matrix X which will be your input matrix where each row is one example. **Remember the intercept term!** (Resulting shape should be (97, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "alpha = 0.01\n",
    "# set number of step of gradient descent\n",
    "interations = 1500\n",
    "# target variables\n",
    "y = data[:, 1]\n",
    "\n",
    "# Please, initliaze the input matrix X\n",
    "X = None\n",
    "print(\"Shape of matrix X: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill in the body of the function `compute_cost(X, y, theta)` which should compute the cost function. Please use vectorized operations rather than loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run our gradient descent algorithm. First, we compute an initial cost. Then in each interations we update our paramenters. We provided a sample loop structure, so you only need to supply the parameter update part. If implemented correctly, your cost should steadily go down and never increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters to zero\n",
    "theta = np.zeros((X.shape[1],))\n",
    "\n",
    "def grad_descent(X, y, theta, alpha):\n",
    "    # number of samples\n",
    "    t = len(y)\n",
    "    \n",
    "    print(\"Initial cost: {}\".format(compute_cost(X, y, theta)))    \n",
    "    history = []\n",
    "    \n",
    "    for i in range(interations):\n",
    "        # Please, fill the following line with parameter update\n",
    "        theta = np.zeros((X.shape[1],))\n",
    "        \n",
    "        cost = compute_cost(X, y, theta)\n",
    "        print(\"Current cost for iteration {}: {}\".format(i, cost))\n",
    "        history.append(cost)\n",
    "        \n",
    "    return theta, history\n",
    "\n",
    "theta, history = grad_descent(X, y, theta, alpha)\n",
    "plt.plot(history)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use your trained parameters to show the linear line they represent with regards to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = np.arange(0, 30)\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "plt.plot(np.array([np.ones(30), line]).T.dot(theta), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two cities, one with 35000 people and second with 70000 thousand people. What would the predicted profits be for each one of them? ***Please, write your answer below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------\n",
    "In the variable `data` we loaded a new *hausing* data for you. The first column in `data` represents the house size (most probably in square feet), the second column represents the number of bedrooms and the last column is the price of the house. As you can see, the house size is generally 1000 times larger that the number of bedrooms. This is undesirable, as  our model will now have to first learn how to approperiatly scale the data, which will most probably cause the training time to be longer or it may even make the model fail to find the underling function (distribution) of the data in question.\n",
    "\n",
    "This can be solved using **feature normalization**, where we make sure that each feature's value has zero mean and unit variance. The usual way how to achive this is to center the values by subtracting the *mean* of the features and and scaling it down by *standard deviation*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('housing_data.csv', delimiter=',')\n",
    "print(\"Shape of loaded data: {}\".format(data.shape))\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "plt.xlabel('House size')\n",
    "plt.ylabel('Number of bedrooms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data (house size, number of bedrooms) into `X` variable and load train target variables into `y`.\n",
    "\n",
    "It may be that you received the data in some sort of an order. This is also not desired, as we would like the model  to rely solely on the data itself (especially in this case) rather than on their order. Shuffling your data should help ensure that (`np.random.permutation` will do in most cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform feature normalization by taking the *mean* and *standard deviation* of our dataset. Each feature should have its own *mean* and *standard deviation*. Note that you need to store these values! Since you want use your model not only on the train data but also on new, previously unseen data. If you only normalized your training data, your model will see the previously unseen data as a completely different distribution and would therefore not be able to handle it. Long story short, you will need to normalize each new example that you would like to predict on using these values.\n",
    "\n",
    "**Do not forget to add intercept term!**\n",
    "\n",
    "\n",
    "#### Do not normalize your target variables! You still want to predict accurate house prices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features HERE\n",
    "\n",
    "print(\"Training set shape: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the same code as before for optimizing parameters. Your code should already be able to run with arbitrary number of features. If not please rewrite `grad_descent` function with matrix operations so it can handle arbitrary amount of features (and not just one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters to zero\n",
    "theta = np.zeros((X.shape[1],))\n",
    "alpha = 0.4\n",
    "\n",
    "theta, history = grad_descent(X, y, theta, alpha)\n",
    "plt.plot(history)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, please compute the value for house with size of 1650 square feet and 3 bedrooms (It should be around 290000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE HERE\n",
    "\n",
    "house = None\n",
    "price = None\n",
    "print(\"House price computed by running gradiant descent: {}\".format(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "In the lecture you also learned that the closed-form solution to linear regression can be described as follows:\n",
    "\n",
    "$$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "This form finds exact solution without any iterations, which is pretty nice.\n",
    "\n",
    "Please provide body of the function `compute_theta_norm_eq` which will return parameters $\\theta$ based on given data in the next cell.\n",
    "\n",
    "\n",
    "Use this function to check if the price of house with 1650 size and 3 bedrooms matches the one found by gradiant descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_norm_eq(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = compute_theta_norm_eq(X, y)\n",
    "\n",
    "price = None\n",
    "print(\"House price computed by normal equations: {}\".format(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print raw values of $\\theta$ what can we conclude about our features? Please, provide brief explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your explation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "Again, in `data` variable we have loaded a new dataset for you. This time it is data related to compressive strength of concrete. Concrete is the most important material in civil engineering and the concrete compressive strength is a highly nonlinear function of age and ingredients. This data consists out of 8 features: \n",
    "\n",
    "* Cement (component 1) - kg in a m3 mixture\n",
    "* Blast Furnace Slag (component 2) - kg in a m3 mixture\n",
    "* Fly Ash (component 3) - kg in a m3 mixture\n",
    "* Water (component 4) - kg in a m3 mixture\n",
    "* Superplasticizer (component 5) - kg in a m3 mixture\n",
    "* Coarse Aggregate (component 6) - kg in a m3 mixture\n",
    "* Fine Aggregate (component 7) - kg in a m3 mixture\n",
    "* Age - Day (1~365)\n",
    "\n",
    "and its target variable:\n",
    "\n",
    "* *Concrete compressive strength - MPa*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('concrete_data.csv', delimiter=',')\n",
    "print(\"Shape of concrete compressive strength data: {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is something we want our model to do, we want it to generalize. But how do we know how well does our model generalize? For this we usually split any dataset we have into three chunks. One is called **train set** and it is used to train the model. The second one, usually taken from the train set is called **validation set** and it is used to optimize hyperparameters of our model (such as learning rate, form of regularization, strength of regularization, etc...). The last chunk of data is called **test set** and it is used **only once**! It is sometimes called also *hold-out* set because we take the data at the beginning and store it somewhere and use it only in the end to see how well our model can *generalize* on previously unseen data.\n",
    "\n",
    "Please split the concrete data (in the `data` variable) into these three sets. Use the provided variables to store the approperiate sets. Split the data in 60:20:20 ratio (60% for training, 20% for validation set, 20% for test set). Resulting shape should be (618, 8) and (206, 8).\n",
    "\n",
    "We provide normalization of the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "\n",
    "X_train = None\n",
    "y_train = None\n",
    "\n",
    "X_val = None\n",
    "y_val = None\n",
    "\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "print(\"Train set X: {} y: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Validation set X: {} y: {}\".format(X_val.shape, y_val.shape))\n",
    "print(\"Test set X: {} y: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data based on traning mean and stadard deviation\n",
    "mean = np.mean(X_train)\n",
    "std = np.std(X_train)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_val = (X_val - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the previously constructed function `compute_theta_norm_eq` to compute new parameters for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = compute_theta_norm_eq(X_train, y_train)\n",
    "print(\"Train MSE error: {}\".format(compute_cost(X_train, y_train, theta)))\n",
    "print(\"Validation MSE error: {}\".format(compute_cost(X_val, y_val, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mention before, this data is nonlinear, which means (among other things) that we can not capture underlining function (distribution) with a simple linear function. This is also suggested by high training error and hight validation error. This phenomenon is also called **underfitting**. There is also oposite phenomenon called **overfitting**, where validation error would be high and training error would be low. This fact often occures when we have too complex model for our task (i.e. a lot of free parameters for optimization and not too much data).\n",
    "\n",
    "However, you may recall from the lectures that linear regression can be generalized to fit more complex functions. We can make simple linear regression into more complex model by adding some more complex features to the input vector. For instance, if we would like to have data of more quadratic nature, we can just make new features by squaring the input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we \"over-do this\", we may end up with an input space that the model can capture too easily (and miss the underlying distribution as a result).\n",
    "\n",
    "Suppose we had data that would be of quadratic nature, and we would have a model that would have utilized features up until their power of 9. We would essentially be searching for a good fitting function in vector space of all functions that can be constructed with polynoms of rank 9. Quadratic function is in this space (all other constants will be 0) but there is low probability that we will find exactly the right function that we need (in our case quadratic). A more likely scenario is that we will find a function that model found first, and that this function will be vastly different from true underlying distribution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to find good fitting features. Use X_train, y_train vectors to enroll features to a satisfing rank, so that the training error would be low and validation would be also low. Please also provide a brief explanation of your setup and approach, and describe in detail when (i.e. at what values of $\\theta$) do you stop experiencing *underfitting* and when do you find you where experiencing *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE HERE\n",
    "\n",
    "theta = compute_theta_norm_eq(X_train, y_train)\n",
    "\n",
    "print(\"Train MSE error: {}\".format(compute_cost(X_train, y_train, theta)))\n",
    "print(\"Validation MSE error: {}\".format(compute_cost(X_val, y_val, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run you model on the test set when you are **sure**, that you found good fitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your final test error: {}\".format(compute_cost(X_test, y_test, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For bonus points:\n",
    "\n",
    "\n",
    "One big disadvatage of normal equations is if we have a big dataset we can not use this to solve for our $\\theta$. Computing the inverse would take really long time, therefore we need to use our gradient descent method.\n",
    "\n",
    "In the variables X, y we loaded some sample data. Run your implementation of gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = utils.give_data()\n",
    "\n",
    "plt.scatter(X[:, 1], X[:, 2])\n",
    "plt.show()\n",
    "\n",
    "theta = np.zeros((3,))\n",
    "alpha = 0.4\n",
    "\n",
    "theta, history = grad_descent(X, y, theta, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your theta coeficients: {}\".format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run \"out-of-the-box\" implementation of linear regression from `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "theta = linear_regression.fit(X, y)\n",
    "\n",
    "print(\"scikit-learn's theta coeficients: {}\".format(theta.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, provide explanation what happend? How is this phenomenom called? And how can this be solved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your explanation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
